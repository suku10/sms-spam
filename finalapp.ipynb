{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install nltk\n",
    "%pip install pandas \n",
    "%pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "f = pd.read_csv(\"sms-spam.csv\")\n",
    "\n",
    "f.shape\n",
    "f\n",
    "Cleaning the data and making it useful\n",
    "f.info()\n",
    "f.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace = True)\n",
    "\n",
    "f\n",
    "f.rename(columns = {'v1':'output', 'v2':'input'}, inplace = True)\n",
    "\n",
    "f\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "f['output'] = encoder.fit_transform(f['output'])\n",
    "\n",
    "f.head()\n",
    "f.isnull().sum()\n",
    "f.duplicated().sum()\n",
    "f = f.drop_duplicates(keep='first')\n",
    "\n",
    "f\n",
    "analysing the data\n",
    "f['output'].value_counts()\n",
    "#calculating spam percentage\n",
    "\n",
    "spam_percentage = (653 / 5169) * 100\n",
    "\n",
    "print(f\"Spam percentage: {spam_percentage:}%\")\n",
    "plt.pie(f['output'].value_counts(),labels = ['NOT SPAM', 'SPAM'], autopct = '%0.2f', radius = 0.9 )\n",
    "\n",
    "plt.show()\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('all')\n",
    "f['countcharacters'] = f['input'].apply(len)\n",
    "\n",
    "f['countwords'] = f['input'].apply(lambda i:len(nltk.word_tokenize(i)))\n",
    "\n",
    "f['countsentences'] = f['input'].apply(lambda i:len(nltk.sent_tokenize(i)))\n",
    "f.head()\n",
    "f[['countcharacters', 'countwords', 'countsentences']].describe()\n",
    "#classifying the data on the basis of spam or not spam\n",
    "\n",
    "#for not spam\n",
    "\n",
    "f[f['output'] == 0][['countcharacters', 'countwords', 'countsentences']].describe()\n",
    "#for spam\n",
    "\n",
    "f[f['output'] == 1][['countcharacters', 'countwords', 'countsentences']].describe()\n",
    "#using seaborn we will plot graph\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "sns.histplot(f[f['output'] == 0]['countcharacters'], color = 'red')#for not spam\n",
    "\n",
    "sns.histplot(f[f['output'] == 1]['countcharacters'], color = 'green')#for spam\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "sns.histplot(f[f['output'] == 0]['countwords'], color = 'red')#for not spam\n",
    "\n",
    "sns.histplot(f[f['output'] == 1]['countwords'], color = 'green')#for spam\n",
    "sns.pairplot(f, hue = 'output')\n",
    "nltk.download('stopwords')\n",
    "def transform_text (text):\n",
    "\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    \n",
    "\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "    removedSC = list()\n",
    "\n",
    "    for i in text:\n",
    "\n",
    "        if i.isalnum():\n",
    "\n",
    "            removedSC.append(i)\n",
    "\n",
    "    text = removedSC[:]\n",
    "\n",
    "    \n",
    "\n",
    "    removedSWPC = list()\n",
    "\n",
    "    for i in text:\n",
    "\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "\n",
    "            removedSWPC.append(i)\n",
    "\n",
    "            \n",
    "\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "    stemmed = list()\n",
    "\n",
    "    for i in text:\n",
    "\n",
    "        stemmed.append(ps.stem(i))\n",
    "\n",
    "    text = stemmed[:]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "f['processed'] = f['input'].apply(transform_text)\n",
    "\n",
    "f.head()\n",
    "wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')\n",
    "\n",
    "#for spam\n",
    "\n",
    "spamWC = wc.generate(f[f['output'] == 1]['processed'].str.cat(sep=\" \"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.imshow(spamWC)\n",
    "#for not spam\n",
    "\n",
    "spamWC = wc.generate(f[f['output'] == 0]['processed'].str.cat(sep=\" \"))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.imshow(spamWC)\n",
    "spamWords = list()\n",
    "\n",
    "\n\n",
    "for msg in f[f['output'] == 1]['processed'].tolist():\n",
    "\n",
    "  for word in msg.split():\n",
    "\n",
    "    spamWords.append(word)\n",
    "\n",
    "spamWords\n",
    "spamWordsDictionary = Counter(spamWords)\n",
    "\n",
    "spamWordsDictionary.most_common(40)\n",
    "mostCommonSPAM = pd.DataFrame(spamWordsDictionary.most_common(40))\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.barplot(data = mostCommonSPAM, x=0, y=1)\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "plt.show()\n",
    "model building\n",
    "#using NaiveBayes classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(f['processed']).toarray()\n",
    "\n",
    "X.shape\n",
    "y = f['output'].values\n",
    "\n",
    "y\n",
    "#splitting the training and testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)\n",
    "#creating the objects for the models\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "#training the dataset for GaussianNB\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "\n",
    "print(precision_score(y_test, y_pred1))\n",
    "#training the dataset for MultinomialnNB\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "\n",
    "print(precision_score(y_test, y_pred2))\n",
    "#training the dataset for BernoulliNB\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "\n",
    "print(precision_score(y_test, y_pred3))\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "X = tf.fit_transform(f['processed']).toarray()\n",
    "\n",
    "y = f['output'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)\n",
    "#training the dataset for GaussianNB\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "\n",
    "print(precision_score(y_test, y_pred1))\n",
    "#training the dataset for MultinomialnNB\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "\n",
    "print(precision_score(y_test, y_pred2))\n",
    "#training the dataset for BernoulliNB\n",
    "\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "\n",
    "print(precision_score(y_test, y_pred3))\n",
    "import pickle\n",
    "\n",
    "pickle.dump(tf,open('vectorizer.pkl','wb'))\n",
    "\n",
    "pickle.dump(mnb,open('spam.pkl','wb'))  # Updated line to save as spam.pkl\n",
    "data training done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</create_file>
